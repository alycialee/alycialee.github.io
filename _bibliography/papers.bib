---
---

@string{aps = {American Physical Society,}}

@article{lee2023scale,
  author={Alycia Lee and Brando Miranda and Sanmi Koyejo},
  journal={ICML Workshop on Data-centric Machine Learning Research and Workshop on Deployable Generative AI},
  title={Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data}, 
  year={2023},
  pdf = {https://dmlr.ai/assets/accepted-papers/113/CameraReady/ICML_2023_DMLR_Workshop__Diversity_Coefficient___LLMs__8pg_.pdf},
  url = {https://arxiv.org/abs/2306.13840},
  abstract = {Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of
model and dataset size. However, the quality of
pre-training data is an important factor for training
powerful LLMs, yet it is a nebulous concept that
has not been fully characterized. Therefore, we
use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects
of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate
that their formal diversity is high when compared
to theoretical lower and upper bounds. In addition,
to build confidence in the diversity coefficient, we
conduct interpretability experiments and find that
the coefficient aligns with intuitive properties of
diversity, e.g., it increases as the number of latent
concepts increases. We conclude the diversity
coefficient is reliable, show it's high for publicly
available LLM datasets, and conjecture it can be
used to build useful diverse datasets for LLMs.},
  code={https://github.com/alycialee/beyond-scale-language-data-diversity/tree/main},
  preview={beyond-scale.png}
}

@article{doi:10.1021/acssynbio.0c00219,
  author = {Wu, Zachary and Yang, Kevin K. and Liszka, Michael J. and Lee, Alycia and Batzilla, Alina and Wernick, David and Weiner, David P. and Arnold, Frances H.},
  title = {Signal Peptides Generated by Attention-Based Neural Networks},
  journal = {ACS Synthetic Biology},
  volume = {9},
  number = {8},
  pages = {2154-2161},
  year = {2020},
  pdf = {https://pubs.acs.org/doi/epdf/10.1021/acssynbio.0c00219},
  url = {https://doi.org/10.1021/acssynbio.0c00219},
  abstract = {Short (15–30 residue) chains of amino acids at the amino termini of expressed proteins known as signal peptides (SPs) specify secretion in living cells. We trained an attention-based neural network, the Transformer model, on data from all available organisms in Swiss-Prot to generate SP sequences. Experimental testing demonstrates that the model-generated SPs are functional: when appended to enzymes expressed in an industrial Bacillus subtilis strain, the SPs lead to secreted activity that is competitive with industrially used SPs. Additionally, the model-generated SPs are diverse in sequence, sharing as little as 58\% sequence identity to the closest known native signal peptide and 73\% ± 9\% on average.},
  preview={signal-peptides.png}
}

@InProceedings{pmlr-v89-yang19c,
  title = 	 {Batched Stochastic Bayesian Optimization via Combinatorial Constraints Design},
  author =       {Yang, Kevin K. and Chen, Yuxin and Lee, Alycia and Yue, Yisong},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3410--3419},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/yang19c/yang19c.pdf},
  url = 	 {https://proceedings.mlr.press/v89/yang19c.html},
  abstract = 	 {In many high-throughput experimental design settings, such as those common in biochemical engineering, batched queries are often more cost effective than one-by-one sequential queries. Furthermore, it is often not possible to directly choose items to query. Instead, the experimenter specifies a set of constraints that generates a library of possible items, which are then selected stochastically. Motivated by these considerations, we investigate \emph{Batched Stochastic Bayesian Optimization} (BSBO), a novel Bayesian optimization scheme for choosing the constraints in order to guide exploration towards items with greater utility. We focus on \emph{site-saturation mutagenesis}, a prototypical setting of BSBO in biochemical engineering, and propose a natural objective function for this problem. Importantly, we show that our objective function can be efficiently decomposed as a difference of submodular functions (DS), which allows us to employ DS optimization tools to greedily identify sets of constraints that increase the likelihood of finding items with high utility. Our experimental results show that our algorithm outperforms common heuristics on both synthetic and two real protein datasets.},
  preview={stochasticBO.png}
}